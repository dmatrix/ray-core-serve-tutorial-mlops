{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214ce2a5-6b11-41d9-86f4-a2b4f0d1a6d9",
   "metadata": {},
   "source": [
    "### Example of Model Composition\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "In this short tutorial we going to use HuggingFace Transformer 🤗 to accomplish three tasks:\n",
    " 1. Analyse the sentiment of a tweet: Positive or Negative\n",
    " 2. Translate it into French\n",
    " 3. Demonstrate the model composition deployment pattern\n",
    " \n",
    " <img src=\"images/sentiment_analysis.jpeg\" width=\"70%\" height=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80759c48-1b18-4ea6-8aa1-6ddbb9083207",
   "metadata": {},
   "source": [
    "#### Install HuggingFace Transformers and Torch modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d229a03-ce6e-4ae8-8857-adcfdc402daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: torch in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: requests in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (2022.4.24)\n",
      "Requirement already satisfied: filelock in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from torch) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23d1ccb-6951-4e22-aafd-48c74c4635de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TranslationPipeline, TextClassificationPipeline\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import requests\n",
    "from ray import serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709b04e-cc7d-4869-8a9f-919d20a2b4cd",
   "metadata": {},
   "source": [
    "These are example 🐦 tweets, some made up, some extracted from a dog lover's twitter handle. In a real use case,\n",
    "these could come live from a Tweeter handle using [Twitter APIs](https://developer.twitter.com/en/docs/twitter-api/getting-started/getting-access-to-the-twitter-api). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "873b8bfd-1ee3-4d9c-bea4-e66924747ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEETS = [\"Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\",\n",
    "          \"Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\",\n",
    "          \"You little dog shit, you peed and pooed on my new carpet. Bad dog!\",\n",
    "          \"I would completely believe you. Dogs and little children - very innocent and open to seeing such things\",\n",
    "          \"You've got too much time on your paws. Go check on the skittle. under the, fridge\",\n",
    "          \"You sneaky little devil, I can't live without you!!!\",\n",
    "          \"It's true what they say about dogs: they are you BEST BUDDY, no matter what!\",\n",
    "          \"This dog is way dope, just can't enough of her\",\n",
    "          \"This dog is way cool, just can't enough of her\",\n",
    "          \"Is a dog really the best pet?\",\n",
    "          \"Cats are better than dogs\",\n",
    "          \"Totally dissastified with the dog. Worst dog ever\",\n",
    "          \"Briliant dog! Reads my moods like a book. Senses my moods and reacts. What a companinon!\"\n",
    "          ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90287e-b3b9-498c-a2fc-fca7169b90fc",
   "metadata": {},
   "source": [
    "Utiliy function to fetch a tweet; these could very well be live tweets coming from Twitter API for a user or a #hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4e6e502-0e23-470d-84f3-fbfdf80f4f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tweet_text(i):\n",
    "    text = TWEETS[i]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc13f240-0412-4f14-943d-98a71d5d2b43",
   "metadata": {},
   "source": [
    "### Sentiment model deployment\n",
    "\n",
    "Our function deployment model to analyse the tweet using a pretrained transformer from HuggingFace 🤗.\n",
    "Note we have number of `replicas=1` but to scale it, we can increase the number of replicas, as\n",
    "we have done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a50cf772-0fa6-4eaf-8288-b1cb5836f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=1)\n",
    "def sentiment_model(text: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer, task=\"sentiment-analysis\")\n",
    "\n",
    "    return pipeline(text)[0]['label'], pipeline(text)[0]['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a5f64-5c28-4af0-b266-4566e1f40c1a",
   "metadata": {},
   "source": [
    "### Translation model deployment\n",
    "\n",
    "Our function to translate a tweet from English --> French using a pretrained Transformer from HuggingFace 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a813a1c8-c577-4028-ad09-8b4dea439e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate a tweet from English --> French \n",
    "# using a pretrained Transformer from HuggingFace\n",
    "@serve.deployment(num_replicas=2)\n",
    "def translate_model(text: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "    model = AutoModelWithLMHead.from_pretrained(\"t5-small\")\n",
    "    use_gpu = 0 if torch.cuda.is_available() else -1\n",
    "    pipeline = TranslationPipeline(model, tokenizer, task=\"translation_en_to_fr\", device=use_gpu)\n",
    "\n",
    "    return pipeline(text)[0]['translation_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9575b2a-11c3-40ea-8522-64ed27fdcbdf",
   "metadata": {},
   "source": [
    "### Use the Model Composition pattern\n",
    "\n",
    "<img src=\"images/tweet_composition.png\" width=\"60%\" height=\"25%\">\n",
    "\n",
    "A composed class is deployed with both sentiment analysis and translations models' ServeHandles initialized in the constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08cec76c-3ef5-4741-914b-edb2c4fd2585",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/composed\", num_replicas=2)\n",
    "class ComposedModel:\n",
    "    def __init__(self):\n",
    "        # fetch and initialize deployment handles\n",
    "        self.translate_model = translate_model.get_handle(sync=False)\n",
    "        self.sentiment_model = sentiment_model.get_handle(sync=False)\n",
    "\n",
    "    async def __call__(self, starlette_request):\n",
    "        data = starlette_request.query_params['data']\n",
    "\n",
    "        sentiment, score = await(await self.sentiment_model.remote(data))\n",
    "        trans_text = await(await self.translate_model.remote(data))\n",
    "\n",
    "        return {'Sentiment': sentiment, 'score': score, 'Translated Text': trans_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1dc58b-d3a4-4ee2-876d-832983ff4642",
   "metadata": {},
   "source": [
    "Start a Ray Serve instance. Note that if Ray cluster does not exist, it will create one and attach the Ray Serve\n",
    "instance to it. If one exists it'll run on that Ray cluster instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acc61315-27e0-4b1b-a4aa-ae6abd2faa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 16:50:41,942\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=47597)\u001b[0m 2022-06-01 16:50:45,829\tINFO checkpoint_path.py:15 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=47597)\u001b[0m 2022-06-01 16:50:45,937\tINFO http_state.py:106 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:WMenKP:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-06-01 16:50:46,664\tINFO api.py:794 -- Started Serve instance in namespace 'serve'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7f88fa5e4be0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26264457-b699-48ff-b5a8-f9a8d66c95f6",
   "metadata": {},
   "source": [
    "### Deploy our models \n",
    "\n",
    "Deploy our models. As seen before in other tutorials, this is as simple and intuitive as invoking `<func_or_class_name>.deploy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b39401d3-8fb0-410c-bb61-d25164fbeab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 16:50:46,679\tINFO api.py:615 -- Updating deployment 'sentiment_model'. component=serve deployment=sentiment_model\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=47605)\u001b[0m INFO:     Started server process [47605]\n",
      "\u001b[2m\u001b[36m(ServeController pid=47597)\u001b[0m 2022-06-01 16:50:46,780\tINFO deployment_state.py:1216 -- Adding 1 replicas to deployment 'sentiment_model'. component=serve deployment=sentiment_model\n",
      "2022-06-01 16:50:48,692\tINFO api.py:630 -- Deployment 'sentiment_model' is ready at `http://127.0.0.1:8000/sentiment_model`. component=serve deployment=sentiment_model\n",
      "2022-06-01 16:50:48,698\tINFO api.py:615 -- Updating deployment 'translate_model'. component=serve deployment=translate_model\n",
      "\u001b[2m\u001b[36m(ServeController pid=47597)\u001b[0m 2022-06-01 16:50:48,779\tINFO deployment_state.py:1216 -- Adding 2 replicas to deployment 'translate_model'. component=serve deployment=translate_model\n",
      "2022-06-01 16:50:50,713\tINFO api.py:630 -- Deployment 'translate_model' is ready at `http://127.0.0.1:8000/translate_model`. component=serve deployment=translate_model\n",
      "2022-06-01 16:50:50,720\tINFO api.py:615 -- Updating deployment 'ComposedModel'. component=serve deployment=ComposedModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=47597)\u001b[0m 2022-06-01 16:50:50,771\tINFO deployment_state.py:1216 -- Adding 2 replicas to deployment 'ComposedModel'. component=serve deployment=ComposedModel\n",
      "2022-06-01 16:50:52,726\tINFO api.py:630 -- Deployment 'ComposedModel' is ready at `http://127.0.0.1:8000/composed`. component=serve deployment=ComposedModel\n"
     ]
    }
   ],
   "source": [
    "sentiment_model.deploy()\n",
    "translate_model.deploy()\n",
    "ComposedModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ea6006-d578-4eaa-9e95-6b2b6d4ff04a",
   "metadata": {},
   "source": [
    "### Send HTTP requests to our deployment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e72dd6-df3b-4236-bd78-d4b0b9bec98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending tweet request... : Tonight on my walk, I got mad because mom wouldn't let me play with this dog. We stared at each other...he never blinked!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:921: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47615)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'POSITIVE', 'score': 0.965121328830719, 'Translated Text': \"Ce soir, j'ai été fou parce que ma mère ne me laisse pas jouer avec ce chien.\"}\n",
      "Sending tweet request... : Sometimes. when i am bored. i will stare at nothing. and try to convince the human. that there is a ghost\n",
      "{'Sentiment': 'NEGATIVE', 'score': 0.99788898229599, 'Translated Text': \"Parfois. quand j'ennuie. je ne regarderai rien. et essayerai de convaincre l'homme.\"}\n",
      "Sending tweet request... : You little dog shit, you peed and pooed on my new carpet. Bad dog!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m - Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m /Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:921: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "\u001b[2m\u001b[36m(translate_model pid=47616)\u001b[0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Sentiment': 'NEGATIVE', 'score': 0.9984055161476135, 'Translated Text': \"Je n'ai pas eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression d'avoir eu l'impression\"}\n",
      "Sending tweet request... : I would completely believe you. Dogs and little children - very innocent and open to seeing such things\n",
      "{'Sentiment': 'POSITIVE', 'score': 0.9997748732566833, 'Translated Text': 'Je vous croyais tout à fait: chiens et petits enfants - très innocents et ouverts à ce genre de choses'}\n",
      "Sending tweet request... : You've got too much time on your paws. Go check on the skittle. under the, fridge\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(TWEETS)):\n",
    "    tweet = fetch_tweet_text(i)\n",
    "    print(F\"Sending tweet request... : {tweet}\")\n",
    "    resp = requests.get(\"http://127.0.0.1:8000/composed\", params={'data': tweet})\n",
    "    print(resp.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6d9b3f-e349-4af6-b9c1-e4ff0a2dab9e",
   "metadata": {},
   "source": [
    "Gracefully shutdown the Ray serve instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786fcf2e-1afa-4900-bc3f-0fc8844d5b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8190d581-6a31-43f2-9083-94578042efdf",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Add more tweets with different sentiments.\n",
    "2. Check the score (and if you speak and read French, what you think of the translation?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff4e209-4437-426e-a425-39b443df68a1",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "1. Instead of French, use a language transformer of your choice\n",
    "2. What about Neutral tweets? Try using [vaderSentiment](https://github.com/cjhutto/vaderSentiment)\n",
    "3. Solution for 2) is [here](https://github.com/anyscale/academy/blob/main/ray-serve/05-Ray-Serve-SentimentAnalysis.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
