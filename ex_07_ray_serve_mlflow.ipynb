{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "674c4a6d-9fbf-46e3-be82-7b41aff25503",
   "metadata": {},
   "source": [
    "# Ray Serve - Integration with Model Registry MLflow\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved\n",
    "\n",
    "This tutorial example shows how to deploy models saved in a model registry such as MLflow to Ray Serve, using the simple Ray Serve deployment APIs. \n",
    "\n",
    "<img src=\"images/serve_mlflow.png\" height=\"50%\" width=\"100%\">\n",
    "\n",
    "You can peruse the saved models' metrics, parameters, and artifacts in MLflow ui.\n",
    "\n",
    "We are going to follow three simple steps:\n",
    "\n",
    "1. Train a scikit-learn classification model\n",
    "2. Use MLflow `autolog()` method to automatically logs all metrics, parameters, artifacts, and the model\n",
    "3. Create a deployment class and deploy the model for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f75277-2913-4c3b-af9f-dc2e02d7f2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from ray import serve\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd875b51-69c0-4974-9af5-8d48da48479d",
   "metadata": {},
   "source": [
    "Define a utility function:\n",
    " * create Iris data set\n",
    " * use a classifier\n",
    " * train and fit model\n",
    " * track all experiments using MLflow `autolog(...)` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9214ce1f-77bb-4df1-b0b3-10affaa8f095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_model():\n",
    "    # load Iris data\n",
    "    iris_data = load_iris()\n",
    "    data, target, target_names = (iris_data['data'],\n",
    "                                  iris_data['target'],\n",
    "                                  iris_data['target_names'])\n",
    "\n",
    "    # Instantiate a model\n",
    "    model = GradientBoostingClassifier()\n",
    "\n",
    "    # Training and validation split\n",
    "    np.random.shuffle(data), np.random.shuffle(target)\n",
    "    train_x, train_y = data[:100], target[:100]\n",
    "    val_x, val_y = data[100:], target[100:]\n",
    "\n",
    "    # Create labels list as file\n",
    "    LABEL_PATH = os.path.join(tempfile.gettempdir(), \"iris_labels.json\")\n",
    "    with open(LABEL_PATH, \"w\") as f:\n",
    "        json.dump(target_names.tolist(), f)\n",
    "\n",
    "    # Train the model and save our label list as an MLflow artifact\n",
    "    # mlflow.sklearn.autolog automatically logs all parameters and metrics during\n",
    "    # the training.\n",
    "    mlflow.sklearn.autolog()\n",
    "    with mlflow.start_run() as run:\n",
    "        model.fit(train_x, train_y)\n",
    "        # Log label list as a artifact\n",
    "        mlflow.log_artifact(LABEL_PATH, artifact_path=\"labels\")\n",
    "    return run.info.run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a399116-0b3e-48d3-8edb-49f149db99ba",
   "metadata": {},
   "source": [
    "Create our Ray Serve deployment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d342d2-e1f0-42fb-b158-dcf4c1b1beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(route_prefix=\"/regressor\")\n",
    "class BoostingModel:\n",
    "    def __init__(self, uri):\n",
    "        # Load the model and label artifact from the local\n",
    "        # Mlflow model registry as a PyFunc Model\n",
    "        self.model = mlflow.pyfunc.load_model(model_uri=uri)\n",
    "\n",
    "        # Download the artifact list of labels\n",
    "        local_dir = \"/tmp/artifact_downloads\"\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.mkdir(local_dir)\n",
    "        client = MlflowClient()\n",
    "        local_path = f\"{client.download_artifacts(run_id, 'labels', local_dir)}/iris_labels.json\"\n",
    "        with open(local_path, \"r\") as f:\n",
    "            self.label_list = json.load(f)\n",
    "\n",
    "    async def __call__(self, starlette_request):\n",
    "        payload = await starlette_request.json()\n",
    "        print(f\"Worker: received Starlette request with data: {payload}\")\n",
    "\n",
    "        # Get the input vector from the payload\n",
    "        input_vector = [\n",
    "            payload[\"sepal length\"],\n",
    "            payload[\"sepal width\"],\n",
    "            payload[\"petal length\"],\n",
    "            payload[\"petal width\"],\n",
    "        ]\n",
    "\n",
    "        # Convert the input vector in a Pandas DataFrame for prediction since\n",
    "        # an MLflow PythonFunc model, model.predict(...), takes pandas DataFrame\n",
    "        prediction = self.model.predict(pd.DataFrame([input_vector]))[0]\n",
    "        human_name = self.label_list[prediction]\n",
    "        return {\"result\": human_name}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dd57e5-14bd-4fce-8975-035f2817ac2c",
   "metadata": {},
   "source": [
    "Train and save the model artifacts in MLflow.\n",
    "Here our MLflow model registry is local file directory `./mlruns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0e9e8c-1275-4d41-8c41-0217f1b2e308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/03/16 16:41:32 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/anaconda3/envs/anyscale-academy/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n"
     ]
    }
   ],
   "source": [
    "run_id = create_and_save_model()\n",
    "# Construct model uri to load the model from our model registry\n",
    "uri = f\"runs:/{run_id}/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86259553-b297-4dbf-93b1-cb8591c59190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-16 16:41:46,204\tINFO services.py:1412 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=64846)\u001b[0m 2022-03-16 16:41:48,386\tINFO checkpoint_path.py:16 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=64846)\u001b[0m 2022-03-16 16:41:48,495\tINFO http_state.py:98 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:iMnufq:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-03-16 16:41:48,711\tINFO api.py:521 -- Started Serve instance in namespace 'serve'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7fd171651310>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start the Ray Serve instance\n",
    "serve.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c152d04d-3a2a-4279-bc37-e41ff5ec85ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=64841)\u001b[0m INFO:     Started server process [64841]\n",
      "2022-03-16 16:41:48,731\tINFO api.py:262 -- Updating deployment 'BoostingModel'. component=serve deployment=BoostingModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=64846)\u001b[0m 2022-03-16 16:41:48,827\tINFO deployment_state.py:920 -- Adding 1 replicas to deployment 'BoostingModel'. component=serve deployment=BoostingModel\n",
      "2022-03-16 16:41:50,215\tINFO api.py:274 -- Deployment 'BoostingModel' is ready at `http://127.0.0.1:8000/regressor`. component=serve deployment=BoostingModel\n"
     ]
    }
   ],
   "source": [
    "# Deploy our model.\n",
    "BoostingModel.deploy(uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1529c81-cd2b-41f5-891f-ca77d2853e21",
   "metadata": {},
   "source": [
    "Send requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50cb75c5-18fa-4d56-9e02-77504eaa68b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send in a request for labels types virginica, setosa, versicolor\n",
    "sample_request_inputs = [{\n",
    "    \"sepal length\": 6.3,\n",
    "    \"sepal width\": 3.3,\n",
    "    \"petal length\": 6.0,\n",
    "    \"petal width\": 2.5\n",
    "    },\n",
    "    {\n",
    "    \"sepal length\": 5.1,\n",
    "    \"sepal width\": 3.5,\n",
    "    \"petal length\": 1.4,\n",
    "    \"petal width\": 0.2\n",
    "    },\n",
    "    {\n",
    "    \"sepal length\": 6.4,\n",
    "    \"sepal width\": 3.2,\n",
    "    \"petal length\": 4.5,\n",
    "    \"petal width\": 1.5},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42aedb74-aea4-485d-b983-5120fc3e10a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": \"setosa\"\n",
      "}\n",
      "{\n",
      "  \"result\": \"virginica\"\n",
      "}\n",
      "{\n",
      "  \"result\": \"versicolor\"\n",
      "}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=64842)\u001b[0m Worker: received Starlette request with data: {'sepal length': 6.3, 'sepal width': 3.3, 'petal length': 6.0, 'petal width': 2.5}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=64842)\u001b[0m Worker: received Starlette request with data: {'sepal length': 5.1, 'sepal width': 3.5, 'petal length': 1.4, 'petal width': 0.2}\n",
      "\u001b[2m\u001b[36m(BoostingModel pid=64842)\u001b[0m Worker: received Starlette request with data: {'sepal length': 6.4, 'sepal width': 3.2, 'petal length': 4.5, 'petal width': 1.5}\n"
     ]
    }
   ],
   "source": [
    "for input_request in sample_request_inputs:\n",
    "    response = requests.get(\"http://localhost:8000/regressor\",\n",
    "                            json=input_request)\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165ad2a-6773-4730-be59-ecee886c06dc",
   "metadata": {},
   "source": [
    "Launch the MLflow UI to see the metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e8874e-7183-4674-bfe6-29442dee56c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-03-16 16:42:51 -0700] [65034] [INFO] Starting gunicorn 20.1.0\n",
      "[2022-03-16 16:42:51 -0700] [65034] [INFO] Listening at: http://127.0.0.1:5000 (65034)\n",
      "[2022-03-16 16:42:51 -0700] [65034] [INFO] Using worker: sync\n",
      "[2022-03-16 16:42:51 -0700] [65038] [INFO] Booting worker with pid: 65038\n",
      "^C\n",
      "[2022-03-16 16:43:02 -0700] [65034] [INFO] Handling signal: int\n",
      "[2022-03-16 16:43:02 -0700] [65038] [INFO] Worker exiting (pid: 65038)\n"
     ]
    }
   ],
   "source": [
    "!mlflow ui "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbae1e2-7dc1-470e-85e5-f36798de8188",
   "metadata": {},
   "source": [
    "### Framework-Specific Tutorials\n",
    "\n",
    "Ray Serve seamlessly integrates with popular Python ML libraries. Below are tutorials with some of these frameworks to help get you started.\n",
    "\n",
    " * [PyTorch Tutorial](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial)\n",
    " * [Scikit-Learn Tutorial](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial)\n",
    " * [Keras and Tensorflow Tutorial](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial)\n",
    " * [Ray Serve MLflow Deployment Pluggin](https://github.com/ray-project/mlflow-ray-serve)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
