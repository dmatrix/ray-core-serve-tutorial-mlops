{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Serve - Model Serving Challenges\n",
    "\n",
    "Â© 2019-2022, Anyscale. All Rights Reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenges of Model Serving\n",
    "\n",
    "Model development happens in a data science research environment. There are many challenges, such as feature engineering, model selection, missing or messy data, yet there are tools at the data scientists' disposal. By contrast, model deployment to production faces an entirely different set of challenges and requires different tools. We must bridge the divide as much as possible.\n",
    "\n",
    "So what are some of the challenges of model serving?\n",
    "\n",
    "<img src=\"https://images.ctfassets.net/xjan103pcp94/6IcTIir1U1WBJdSbdygQ08/70ceeb0e4f5c8b72b7007c61cb19eed8/WhereRayServeFitsIn.png\" width=\"70%\" height=\"40%\">\n",
    "\n",
    "### 1. It Should Be Framework Agnostic\n",
    "\n",
    "First, model serving frameworks must be able to serve models from popular frameworks and libraries like TensorFlow, PyTorch, scikit-learn, or even arbitrary Python functions. Even within the same organization, it is common to use several machine learning frameworks, in order to get the best model. \n",
    "\n",
    "Second, machine learning models are typically surrounded by (or work in conjunction with) \n",
    "lots of application or business logic. For example, some model serving is implemented as a RESTful service to which scoring requests are made. Often this is too restrictive, as some additional processing, such as fetching additional data from a online feature store, to augment the request data, may be desired as part of the scoring process, and the performance overhead of remote calls may be suboptimal.\n",
    "\n",
    "### 2. Pure Python or Pythonic\n",
    "\n",
    "In general, model serving should be intuitive for developers and simple to configure and run. Hence, it is desirable to use pure Python and to avoid verbose configurations using YAML files or other means. \n",
    "\n",
    "Data scientists and engineers use Python and Python-based ML frameworks to develop their machine learning models, so they should also be able to use Python to deploy their machine learning applications. This need is growing more critical as online learning applications combine training and serving in the same applications.\n",
    "\n",
    "### 3. Simple and Scalable\n",
    "\n",
    "Model serving must be simple to scale on demand across many machines. It must also be easy to upgrade models dynamically, over time. Achieving production uptime and performance requirements are essential for success.\n",
    "\n",
    "### 4. DevOps/MLOps Integrations\n",
    "\n",
    "Model serving deployments need to integrate with existing \"DevOps\" CI/CD practices for controlled, audited, and predicatble releases. Patterns like [Canary Deployment](https://martinfowler.com/bliki/CanaryRelease.html) are particularly useful for testing the efficacy of a new model before replacing existing models, just as this pattern is useful for other software deployments.\n",
    "\n",
    "### 5. Flexible Deployment Patterns\n",
    "\n",
    "There are unique deployment patterns, too. For example, it should be easy to deploy a forest of models, to split traffic to different instances, and to score data in batches for greater efficiency.\n",
    "\n",
    "See also this [Ray blog post](https://medium.com/distributed-computing-with-ray/the-simplest-way-to-serve-your-nlp-model-in-production-with-pure-python-d42b6a97ad55) on the challenges of model serving and the way Ray Serve addresses them. It also provides an example of starting with a simple model, then deploying a more sophisticated model into the running application. Along the same lines, this blog post, [Serving ML Models in Production Common Patterns](https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns) discusses how deployment patterns for model serving and how you can use Ray Serve. Additionally, listen to this webinar: [Building a scalable ML model serving API with Ray Serve](https://www.anyscale.com/events/2021/09/09/building-a-scalable-ml-model-serving-api-with-ray-serve). This introductory webinar highlights how Ray Serve makes it easy to deploy, operate and scale a machine learning API.\n",
    "\n",
    "<img src=\"images/PatternsMLProduction.png\" width=\"70%\" height=\"40%\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Ray Serve?\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) is a scalable, framework-agnostic and Python-first model serving library built on [Ray](https://ray.io).\n",
    "\n",
    "<img src=\"images/ray_serve_overview.png\" width=\"70%\" height=\"40%\"> \n",
    "\n",
    "For users, Ray Serve offers these benefits:\n",
    "\n",
    "* **Framework Agnostic**: You can use the same toolkit to serve everything from deep learning models built with [PyTorch](https://docs.ray.io/en/latest/serve/tutorials/pytorch.html#serve-pytorch-tutorial), [Tensorflow](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), or [Keras](https://docs.ray.io/en/latest/serve/tutorials/tensorflow.html#serve-tensorflow-tutorial), to [scikit-Learn](https://docs.ray.io/en/latest/serve/tutorials/sklearn.html#serve-sklearn-tutorial) models, to arbitrary business logic.\n",
    "* **Python First:** Configure your model serving with pure Python code. No YAML or JSON configurations required.\n",
    "\n",
    "Since Serve is built on Ray, it also allows you to scale to many machines, in your datacenter or in cloud environments, and it allows you to leverage all of the other Ray frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ray Serve Architecture and components\n",
    "\n",
    "<img src=\"images/architecture.png\" height=\"40%\" width=\"70%\">\n",
    "\n",
    "There are three kinds of actors that are created to make up a Serve instance:\n",
    "\n",
    "**Controller**: A global actor unique to each Serve instance that manages the control plane. The Controller is responsible for creating, updating, and destroying other actors. Serve API calls like creating or getting a deployment make remote calls to the Controller.\n",
    "\n",
    "**Router**: There is one router per node. Each router is a Uvicorn HTTP server that accepts incoming requests, forwards them to replicas, and responds once they are completed.\n",
    "\n",
    "**Worker Replica**: Worker replicas actually execute the code in response to a request. For example, they may contain an instantiation of an ML model. Each replica processes individual requests from the routers (they may be batched by the replica using `@serve.batch`, see the [batching docs](https://docs.ray.io/en/latest/serve/ml-models.html#serve-batching)).\n",
    "\n",
    "For more details, see this [key concepts](https://docs.ray.io/en/latest/serve/index.html) and [architecture](https://docs.ray.io/en/latest/serve/architecture.html) documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifetime of a Request\n",
    "\n",
    "When an HTTP request is sent to the router, the following things happen:\n",
    "\n",
    " * The HTTP request is received and parsed.\n",
    "\n",
    " * The correct deployment associated with the HTTP url path is looked up. The request is placed on a queue.\n",
    "\n",
    " * For each request in a deployment queue, an available replica is looked up and the request is sent to it. If there are no available replicas (there are more than max_concurrent_queries requests outstanding), the request is left in the queue until an outstanding request is finished.\n",
    "\n",
    "Each replica maintains a queue of requests and executes one at a time, possibly using asyncio to process them concurrently. If the handler (the function for the deployment or __call__) is async, the replica will not wait for the handler to run; otherwise, the replica will block until the handler returns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Simple Ray Serve Examples\n",
    "\n",
    "We'll explore a more detailed example later in this notebook, where we actually serve ML models. Here we explore how deployments are simple with Ray Serve! We will first use a function that does \"scoring,\" sufficient for _stateless_ scenarios, then use a class, which enables _stateful_ scenarios.\n",
    "\n",
    "<img src=\"images/func_class_deployment.png\" width=\"80%\" height=\"50%\">\n",
    "\n",
    "But first, initialize Ray as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "\n",
    "import requests  # for making web requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize Ray Serve itself. Note that we did not have to start a Ray cluster explicity. If one is not running `serve.start()` will automatically launch a Ray cluster, otherwise it'll connect to an exisisting instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 13:48:15,513\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(ServeController pid=43769)\u001b[0m 2022-06-09 13:48:18,677\tINFO checkpoint_path.py:15 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=43769)\u001b[0m 2022-06-09 13:48:18,782\tINFO http_state.py:106 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:EnVZZA:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-06-09 13:48:19,518\tINFO api.py:794 -- Started Serve instance in namespace 'serve'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=43773)\u001b[0m INFO:     Started server process [43773]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.serve.api.Client at 0x7faa0fedc220>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define our stateless function for processing requests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple function that will be served by Ray. As with Ray Tasks, we can decorate this function with `@serve.deployment`, meaning this is going to be\n",
    "deployed on Ray Serve as function to which we can send Starlette requests.\n",
    "\n",
    "It takes in a `request`, extracts the request parameter with key \"name,\"\n",
    "and returns an echoed string. \n",
    "\n",
    "Simple to illustrate that Ray Serve can also serve Python functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Python function deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "def hello(request):\n",
    "    name = request.query_params[\"name\"]\n",
    "    return f\"Hello {name}!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `<func_name>.deploy()` method to deploy it on Ray Serve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy a Python function for serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 13:48:19,541\tINFO api.py:615 -- Updating deployment 'hello'. component=serve deployment=hello\n",
      "\u001b[2m\u001b[36m(ServeController pid=43769)\u001b[0m 2022-06-09 13:48:19,618\tINFO deployment_state.py:1216 -- Adding 1 replicas to deployment 'hello'. component=serve deployment=hello\n",
      "2022-06-09 13:48:20,548\tINFO api.py:630 -- Deployment 'hello' is ready at `http://127.0.0.1:8000/hello`. component=serve deployment=hello\n"
     ]
    }
   ],
   "source": [
    "hello.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send some requests to our Python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: Hello request_0!\n",
      " 1: Hello request_1!\n",
      " 2: Hello request_2!\n",
      " 3: Hello request_3!\n",
      " 4: Hello request_4!\n",
      " 5: Hello request_5!\n",
      " 6: Hello request_6!\n",
      " 7: Hello request_7!\n",
      " 8: Hello request_8!\n",
      " 9: Hello request_9!\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    response = requests.get(f\"http://127.0.0.1:8000/hello?name=request_{i}\").text\n",
    "    print(f'{i:2d}: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see `hello request_N` in the output. \n",
    "\n",
    "Now let's serve another \"model\" in the same Ray Serve instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import starlette\n",
    "from starlette.requests import Request\n",
    "\n",
    "@serve.deployment\n",
    "class SimpleModel:\n",
    "    def __init__(self):\n",
    "        self.weight = 0.5\n",
    "        self.bias = 1\n",
    "        self.prediction = 0.0\n",
    "\n",
    "    def predict(self, data):\n",
    "        self.prediction = float(data) * self.weight * random() + self.bias\n",
    "        return {\"prediction\": self.prediction}\n",
    "    \n",
    "    def __call__(self, starlette_request):\n",
    "        return self.predict(starlette_request.query_params['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 13:48:20,642\tINFO api.py:615 -- Updating deployment 'SimpleModel'. component=serve deployment=SimpleModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=43769)\u001b[0m 2022-06-09 13:48:20,677\tINFO deployment_state.py:1216 -- Adding 1 replicas to deployment 'SimpleModel'. component=serve deployment=SimpleModel\n",
      "2022-06-09 13:48:21,649\tINFO api.py:630 -- Deployment 'SimpleModel' is ready at `http://127.0.0.1:8000/SimpleModel`. component=serve deployment=SimpleModel\n"
     ]
    }
   ],
   "source": [
    "SimpleModel.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Send some requests to our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction  : Task Error. Traceback: \u001b[36mray::RayServeWrappedReplica.handle_request()\u001b[39m (pid=43779, ip=127.0.0.1)\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/utils.py\", line 243, in wrap_to_ray_error\n",
      "    raise exception\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/replica.py\", line 396, in invoke_single\n",
      "    result = await method_to_call(*args, **kwargs)\n",
      "  File \"/var/folders/75/7kpb8gj57k11df4ksj9lnt900000gn/T/ipykernel_43687/3639668165.py\", line 17, in __call__\n",
      "TypeError: predict() takes 1 positional argument but 2 were given.\n",
      "prediction  : Task Error. Traceback: \u001b[36mray::RayServeWrappedReplica.handle_request()\u001b[39m (pid=43779, ip=127.0.0.1)\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/utils.py\", line 243, in wrap_to_ray_error\n",
      "    raise exception\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/replica.py\", line 396, in invoke_single\n",
      "    result = await method_to_call(*args, **kwargs)\n",
      "  File \"/var/folders/75/7kpb8gj57k11df4ksj9lnt900000gn/T/ipykernel_43687/3639668165.py\", line 17, in __call__\n",
      "TypeError: predict() takes 1 positional argument but 2 were given.\n",
      "prediction  : Task Error. Traceback: \u001b[36mray::RayServeWrappedReplica.handle_request()\u001b[39m (pid=43779, ip=127.0.0.1)\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/utils.py\", line 243, in wrap_to_ray_error\n",
      "    raise exception\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/replica.py\", line 396, in invoke_single\n",
      "    result = await method_to_call(*args, **kwargs)\n",
      "  File \"/var/folders/75/7kpb8gj57k11df4ksj9lnt900000gn/T/ipykernel_43687/3639668165.py\", line 17, in __call__\n",
      "TypeError: predict() takes 1 positional argument but 2 were given.\n",
      "prediction  : Task Error. Traceback: \u001b[36mray::RayServeWrappedReplica.handle_request()\u001b[39m (pid=43779, ip=127.0.0.1)\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/utils.py\", line 243, in wrap_to_ray_error\n",
      "    raise exception\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/replica.py\", line 396, in invoke_single\n",
      "    result = await method_to_call(*args, **kwargs)\n",
      "  File \"/var/folders/75/7kpb8gj57k11df4ksj9lnt900000gn/T/ipykernel_43687/3639668165.py\", line 17, in __call__\n",
      "TypeError: predict() takes 1 positional argument but 2 were given.\n",
      "prediction  : Task Error. Traceback: \u001b[36mray::RayServeWrappedReplica.handle_request()\u001b[39m (pid=43779, ip=127.0.0.1)\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/utils.py\", line 243, in wrap_to_ray_error\n",
      "    raise exception\n",
      "  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/replica.py\", line 396, in invoke_single\n",
      "    result = await method_to_call(*args, **kwargs)\n",
      "  File \"/var/folders/75/7kpb8gj57k11df4ksj9lnt900000gn/T/ipykernel_43687/3639668165.py\", line 17, in __call__\n",
      "TypeError: predict() takes 1 positional argument but 2 were given.\n"
     ]
    }
   ],
   "source": [
    "url = f\"http://127.0.0.1:8000/SimpleModel\"\n",
    "for i in range(5):\n",
    "    print(f\"prediction  : {requests.get(url, params={'data': random()}).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': Deployment(name=hello,version=None,route_prefix=/hello),\n",
       " 'SimpleModel': Deployment(name=SimpleModel,version=None,route_prefix=/SimpleModel)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve.list_deployments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how to deploy ML models and query them via two methods:\n",
    " 1. ServeHandle API gives you control and a pythonic interface to your deployments\n",
    " 2. HTTP offers an HTTP client and web interface to access your deployments. This could be suitable for web application sending an HTTP request to your model deployment \n",
    " <img src=\"images/func_class_deployment_2.png\" width=\"80%\" height=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example model stored in a pickled format at an accessible path in the cloud storage or model registry\n",
    "that can be reloaded and deserialized into a model instance. Once deployed\n",
    "in Ray Serve, we can use it for prediction. The prediction is a fake condition,\n",
    "based on threshold of weight greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def predict(self, data):\n",
    "        return random() + data if data > 0.5 else data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "@serve.deployment\n",
    "class Deployment:\n",
    "    # Take in a path to load your desired model\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.path = path\n",
    "        self.model = Model(path)\n",
    "        # Get the pid on which this deployment is running on\n",
    "        self.pid = os.getpid()\n",
    "\n",
    "    # Deployments are callable. Here we simply return a prediction from\n",
    "    # our request\n",
    "    def predict(self, data) -> str:\n",
    "        pred = self.model.predict(float(data))\n",
    "        return f\"(pid: {self.pid}); path: {self.path}; data: {float(data):.3f}; prediction: {pred:.3f}\"\n",
    "\n",
    "    def __call__(self, starlette_request) -> str:\n",
    "        return self.predict(starlette_request.query_params['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two distinct deployments of the same class as two replicas. \n",
    "Associate each deployment with a unique 'name'. This name can be used to fetch its respective ServeHandle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 13:48:21,720\tINFO api.py:615 -- Updating deployment 'rep-1'. component=serve deployment=rep-1\n",
      "\u001b[2m\u001b[36m(ServeController pid=43769)\u001b[0m 2022-06-09 13:48:21,724\tINFO deployment_state.py:1216 -- Adding 2 replicas to deployment 'rep-1'. component=serve deployment=rep-1\n",
      "2022-06-09 13:48:22,727\tINFO api.py:630 -- Deployment 'rep-1' is ready at `http://127.0.0.1:8000/rep-1`. component=serve deployment=rep-1\n",
      "2022-06-09 13:48:22,731\tINFO api.py:615 -- Updating deployment 'rep-2'. component=serve deployment=rep-2\n",
      "\u001b[2m\u001b[36m(ServeController pid=43769)\u001b[0m 2022-06-09 13:48:22,778\tINFO deployment_state.py:1216 -- Adding 2 replicas to deployment 'rep-2'. component=serve deployment=rep-2\n",
      "2022-06-09 13:48:23,739\tINFO api.py:630 -- Deployment 'rep-2' is ready at `http://127.0.0.1:8000/rep-2`. component=serve deployment=rep-2\n"
     ]
    }
   ],
   "source": [
    "Deployment.options(name=\"rep-1\", num_replicas=2).deploy(\"/model/rep-1.pkl\")\n",
    "Deployment.options(name=\"rep-2\", num_replicas=2).deploy(\"/model/rep-2.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List deployments again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': Deployment(name=hello,version=None,route_prefix=/hello), 'SimpleModel': Deployment(name=SimpleModel,version=None,route_prefix=/SimpleModel), 'rep-1': Deployment(name=rep-1,version=None,route_prefix=/rep-1), 'rep-2': Deployment(name=rep-2,version=None,route_prefix=/rep-2)}\n"
     ]
    }
   ],
   "source": [
    "print(serve.list_deployments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Access each deployment using the ServeHandle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-09 13:48:23,755\tWARNING api.py:485 -- You are retrieving a sync handle inside an asyncio loop. Try getting client.get_handle(.., sync=False) to get better performance. Learn more at https://docs.ray.io/en/master/serve/http-servehandle.html#sync-and-async-handles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handle name : rep-1\n"
     ]
    },
    {
     "ename": "RayTaskError(NameError)",
     "evalue": "\u001b[36mray::RayServeWrappedReplica.handle_request()\u001b[39m (pid=43788, ip=127.0.0.1)\n  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/utils.py\", line 243, in wrap_to_ray_error\n    raise exception\n  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/replica.py\", line 400, in invoke_single\n    result = await method_to_call()\n  File \"/var/folders/75/7kpb8gj57k11df4ksj9lnt900000gn/T/ipykernel_43687/470044568.py\", line 15, in predict\nNameError: name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(NameError)\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m handle \u001b[38;5;241m=\u001b[39m serve\u001b[38;5;241m.\u001b[39mget_deployment(d_name)\u001b[38;5;241m.\u001b[39mget_handle()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhandle name : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction  : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mray\u001b[38;5;241m.\u001b[39mget(handle\u001b[38;5;241m.\u001b[39mpredict\u001b[38;5;241m.\u001b[39mremote(random()))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/worker.py:1809\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1807\u001b[0m     worker\u001b[38;5;241m.\u001b[39mcore_worker\u001b[38;5;241m.\u001b[39mdump_object_store_memory_usage()\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayTaskError):\n\u001b[0;32m-> 1809\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mRayTaskError(NameError)\u001b[0m: \u001b[36mray::RayServeWrappedReplica.handle_request()\u001b[39m (pid=43788, ip=127.0.0.1)\n  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/utils.py\", line 243, in wrap_to_ray_error\n    raise exception\n  File \"/Users/archit/anaconda3/envs/ray-core-serve-tutorial/lib/python3.8/site-packages/ray/serve/replica.py\", line 400, in invoke_single\n    result = await method_to_call()\n  File \"/var/folders/75/7kpb8gj57k11df4ksj9lnt900000gn/T/ipykernel_43687/470044568.py\", line 15, in predict\nNameError: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for d_name in [\"rep-1\", \"rep-2\"]:\n",
    "        # Get handle to the each deployment and invoke its method.\n",
    "        # Which replica the request is dispatched to is determined\n",
    "        # by the Router actor.\n",
    "        handle = serve.get_deployment(d_name).get_handle()\n",
    "        print(f\"handle name : {d_name}\")\n",
    "        print(f\"prediction  : {ray.get(handle.predict.remote(random()))}\")\n",
    "        print(\"-\" * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Access deployment via HTTP Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(2):\n",
    "    for d_name in [\"rep-1\", \"rep-2\"]:\n",
    "        # Send HTTP request along with data payload\n",
    "        url = f\"http://127.0.0.1:8000/{d_name}\"\n",
    "        print(f\"handle name : {d_name}\")\n",
    "        print(f\"prediction  : {requests.get(url, params={'data': random()}).text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut down Ray Serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise (Optional)\n",
    "\n",
    "Here are some things you can try:\n",
    "\n",
    "1. For each of Method 1 and Method 2, send ten requests\n",
    "2. Increase the number of replicas\n",
    "3. Do requests get sent to different replicas? (check the pids or the Ray Dashboard)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7870826f35578f78988c006074c778c7fb98b01979d0d06d7a011367199c39c2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
